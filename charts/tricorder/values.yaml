# Default values for tricorder.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.

imagePullSecrets: []
nameOverride: ""
fullnameOverride: ""

starship:
  serviceAccount:
    # Specifies whether a service account should be created
    create: true
    # Annotations to add to the service account
    annotations: {}
    # The name of the service account to use.
    # If not set and create is true, a name is generated using the fullname template
    name: "starship"

  replicaCount: 1
  service:
    type: ClusterIP
    port: 80

  apiServer:
    image:
      repository: public.ecr.aws/w3q2q1s5/api-server
      pullPolicy: Always
      # Overrides the image tag whose default is the chart appVersion.
      tag: "v0.1-2022-12-22_17-17"

    command:
      extraArgs:
        - --module_db_path=/tricorder/http/tricorder.db
        - --pg_url=postgresql://postgres:tricorder@timescaledb:5432/tricorder

    ports:
      serverhttp:
        enabled: true
        containerPort: 8080
        servicePort: 8080
        hostPort: 8080
        protocol: TCP
      grpc:
        enabled: true
        containerPort: 50051
        servicePort: 50051
        hostPort: 50051
        protocol: TCP

    resources:
      limits:
        cpu: 400m
        memory: 500Mi
      requests:
        cpu: 100m
        memory: 128Mi

  agent:
    image:
      repository: public.ecr.aws/w3q2q1s5/tricorder-agent
      pullPolicy: IfNotPresent
      # Overrides the image tag whose default is the chart appVersion.
      tag: "v0.1-2022-12-22_17-17"

  ui:
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi
    image:
      repository: public.ecr.aws/w3q2q1s5/mgmt-ui
      pullPolicy: IfNotPresent
      # Overrides the image tag whose default is the chart appVersion.
      tag: "v0.1-2022-12-22_17-17"

    ports:
      uihttp:
        enabled: true
        containerPort: 80
        servicePort: 80
        hostPort: 80
        protocol: TCP
    resources:
      limits:
        cpu: 100m
        memory: 128Mi
      requests:
        cpu: 100m
        memory: 128Mi

timescaledb-single:
  enabled: true
  # This file and its contents are licensed under the Apache License 2.0.
  # Please see the included NOTICE for copyright information and LICENSE for a copy of the license.

  replicaCount: 1

  # To prevent very long names, we override the name, otherwise it would default to
  # timescaledb-single (the name of the chart)
  fullnameOverride: timescaledb
  #clusterName: "{{ .Release.Name }}-tsdb"

  # By default those secrets are randomly generated.
  # To prevent misconfiguration, modifications from helm upgrade won't be applied to those secrets.
  # As a result changing secrets cannot be done via helm and need manual intervention.
  secrets:
    # This map should contain environment variables that influence Patroni,
    # for example PATRONI_SUPERUSER_PASSWORD or PATRONI_REPLICATION_PASSWORD
    # https://patroni.readthedocs.io/en/latest/ENVIRONMENT.html#postgresql
    credentials:
      PATRONI_SUPERUSER_PASSWORD: "tricorder"
      PATRONI_REPLICATION_PASSWORD: "tricorder"
      PATRONI_admin_PASSWORD: "tricorder"

    # This secret should contain environment variables that influence pgBackRest.
    pgbackrest:
      PGBACKREST_REPO1_S3_REGION: ""
      PGBACKREST_REPO1_S3_KEY: ""
      PGBACKREST_REPO1_S3_KEY_SECRET: ""
      PGBACKREST_REPO1_S3_BUCKET: ""
      PGBACKREST_REPO1_S3_ENDPOINT: "s3.amazonaws.com"

    # Selector used to provision your own Secret containing pgbackrest configuration details
    # This is mutually exclusive with `pgbackrest` option and takes precedence over it.
    # WARNING: Use this option with caution
    pgbackrestSecretName: ""

  backup:
    enabled: false
    pgBackRest:
      # https://pgbackrest.org/configuration.html
      # Although not impossible, care should be taken not to include secrets
      # in these parameters. Use Kubernetes Secrets to specify S3 Keys, Secrets etc.
      compress-type: lz4
      process-max: 4
      start-fast: "y"
      repo1-retention-diff: 2
      repo1-retention-full: 2
      repo1-type: s3
      repo1-cipher-type: "none"
      repo1-s3-region: us-east-2
      repo1-s3-endpoint: s3.amazonaws.com

    # Overriding the archive-push/archive-get sections is most useful in
    # very high througput situations. Look at values/high_throuhgput_example.yaml for more details
    pgBackRest:archive-push: {}
    pgBackRest:archive-get: {}
    jobs:
        # name: needs to adhere to the kubernetes restrictions
        # type: can be full, incr or diff, see https://pgbackrest.org/user-guide.html
        # schedule: https://en.wikipedia.org/wiki/Cron#CRON_expression
      - name: full-weekly
        type: full
        schedule: "12 02 * * 0"
      - name: incremental-daily
        type: incr
        schedule: "12 02 * * 1-6"
    # Extra custom environment variables for the backup container.
    envFrom:
    # - secretRef:
    #     name: extra-pgbackrest-secrets

    # Alternatively, you can expose individual environment variables:
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
    # Although not impossible, care should be taken not to include secrets
    # in these parameters. Use Kubernetes Secrets to specify S3 Keys, Secrets etc.
    env:
    # - name: PGBACKREST_REPO1_S3_BUCKET
    #   value: my_example_s3_bucket_for_backups
    # - name: PGBACKREST_REPO1_S3_KEY_SECRET
    #   valueFrom:
    #     secretKeyRef:
    #       name: pgbackrest-dev-secrets
    #       key: repo1-s3-key-secret
    resources: {}
    # If you do want to specify resources, uncomment the following
    # lines, adjust them as necessary, and remove the curly braces after 'resources:'.
    # limits:
    #   cpu: 100m
    #   memory: 128Mi
    # requests:
    #   cpu: 100m
    #   memory: 128Mi

  # When creating a *new* deployment, the default is to initialize (using initdb) the database.
  # If however, you want to initialize the database using an existing backup, you can do so by
  # configuring this section.
  #
  # WARNING: You *should not* run 2 identically named deployments in separate Kubernetes
  #          clusters using the same S3 bucket for backups.
  bootstrapFromBackup:
    enabled: false
    # Setting the s3 path is mandatory to avoid overwriting an already existing backup,
    # and to be sure the restore is explicitly the one requested.
    repo1-path:
    # Here you can (optionally) provide a Secret to configure the restore process further.
    # For example, if you need to specify a different restore bucket, you should set
    # PGBACKREST_REPO1_S3_BUCKET: <base64 encoded value of the bucket> in these secrets
    secretName: pgbackrest-bootstrap


  # Extra custom environment variables.
  # These should be an EnvVar, as this allows you to inject secrets into the environment
  # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
  env:
  #  - name: NOT_A_SECRET
  #    value: "test"
  #  - name: MYAPPLICATION_STANDBY_PASSWORDS
  #    valueFrom:
  #      secretKeyRef:
  #        name: myapplication-passwords
  #        key: standby

  # Externally created Kubernetes secrets will be injected into the pods by referencing them here. You
  # can also add more configuration options and secrets this way (see https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/#configure-all-key-value-pairs-in-a-configmap-as-container-environment-variables)
  envFrom:
  #  - configMapRef:
  #      name: my-deployment-settings
  #      optional: true


  # This configuration will be passed on to Patroni directly, there are a few things that are
  # injected/changed, these are:
  #   - archive_command will be set to /bin/true if backup is disabled
  #   - any context sensitive parameter (scope, namespace, name) will be overridden by the Kubernetes context
  # https://patroni.readthedocs.io/en/latest/SETTINGS.html#settings
  patroni:
    bootstrap:
      dcs:
        postgresql:
          parameters:
            checkpoint_timeout: 15min
            bgwriter_delay: 10ms
            bgwriter_lru_maxpages: 100000
            # synchronous_commit: "off"
            shared_preload_libraries: timescaledb,pg_stat_statements,pg_stat_monitor,pg_stat_kcache
            # TODO: stats_temp_directory was deprectated on PG15, we should remove it when the DB upgrades.
            stats_temp_directory: /tmp/pg_stat_temp
  postInit:
    # A list of sources, that contain post init scripts.
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#projectedvolumesource-v1-core
    # These scripts are all projected to the same directory and will be executed
    # in sorted order only once: After a cluster initialization
    # Some examples:
    - configMap:
        name: tricorder-db-post-init
    - secret:
        name: custom-secret-scripts

  # Values for defining the primary & replica Kubernetes Services.
  service:
    primary:
      # One of (ClusterIP | LoadBalancer | NodePort). Headless services are not supported.
      type: ClusterIP
      # The port used by the service.
      port: 5432
      # Additional labels to be added to the Service.
      labels: {}
      # Additional annotations to be added to the Service.
      annotations: {}
      # Define extra fields to be interpolated into the Service spec.
      #
      # This allows for adding support for new features and functionality which may not yet
      # be directly supported in this chart.
      spec: {}
      # loadBalancerSourceRanges:
      # - "0.0.0.0/0"

    replica:
      # One of (ClusterIP | LoadBalancer | NodePort). Headless services are not supported.
      type: ClusterIP
      # The port used by the service.
      port: 5432
      # Additional labels to be added to the Service.
      labels: {}
      # Additional annotations to be added to the Service.
      annotations: {}
      # Define extra fields to be interpolated into the Service spec.
      #
      # This allows for adding support for new features and functionality which may not yet
      # be directly supported in this chart.
      spec: {}
      # loadBalancerSourceRanges:
      # - "0.0.0.0/0"

  readinessProbe:
    enabled: true
    initialDelaySeconds: 5
    periodSeconds: 30
    timeoutSeconds: 5
    failureThreshold: 6
    successThreshold: 1

  persistentVolumes:
    # For sanity reasons, the actual PGDATA and wal directory will be subdirectories of the Volume mounts,
    # this allows Patroni/a human/an automated operator to move directories during bootstrap, which cannot
    # be done if we did not use subdirectories
    # https://www.postgresql.org/docs/current/creating-cluster.html#CREATING-CLUSTER-MOUNT-POINTS
    data:
      enabled: true
      size: 2Gi
      ## database data Persistent Volume Storage Class
      ## If defined, storageClassName: <storageClass>
      ## If set to "-", storageClassName: "", which disables dynamic provisioning
      ## If undefined (the default) or set to null, no storageClassName spec is
      ##   set, choosing the default provisioner.  (gp2 on AWS, standard on
      ##   GKE, AWS & OpenStack)
      ##
      storageClass: "local"
      subPath: ""
      mountPath: "/var/lib/postgresql"
      annotations: {}
      accessModes:
        - ReadWriteOnce
    # WAL will be a subdirectory of the data volume, which means enabling a separate
    # volume for the WAL files should just work for new pods.
    wal:
      enabled: true
      size: 1Gi
      subPath: ""
      storageClass: "local"
      # When changing this mountPath ensure you also change the following key to reflect this:
      # patroni.postgresql.basebackup.[].waldir
      mountPath: "/var/lib/postgresql/wal"
      annotations: {}
      accessModes:
        - ReadWriteOnce
    # Any tablespace mentioned here requires a volume that will be associated with it.
    # tablespaces:
      # example1:
      #   size: 5Gi
      #   storageClass: gp2
      # example2:
      #   size: 5Gi
      #   storageClass: gp2

  resources:
    requests:
      cpu: 100m
      memory: 2Gi

  sharedMemory:
    useMount: false

  # timescaledb-tune will be run with the Pod resources requests or - if not set - its limits.
  # This should give a reasonably tuned PostgreSQL instance.
  # Any PostgreSQL parameter that is explicitly set in the Patroni configuration will override
  # the auto-tuned variables.
  timescaledbTune:
    enabled: true
    # For full flexibility, we allow you to override any timescaledb-tune parameter below.
    # However, these parameters only take effect on newly scheduled pods and their settings are
    # only visibible inside those new pods.
    # Therefore you probably want to set explicit overrides in patroni.bootstrap.dcs.postgresql.parameters,
    # as those will take effect as soon as possible.
    # https://github.com/timescale/timescaledb-tune
    args: {}
      # max-conns: 120
      # cpus: 5
      # memory: 4GB

  # pgBouncer does connection pooling for PostgreSQL
  # https://www.pgbouncer.org/
  # enabling pgBouncer will run an extra container in every Pod, serving a pgBouncer
  # pass-through instance
  pgBouncer:
    enabled: false
    port: 6432
    config:
    # DANGER: The below settings are considered to be safe to set, and we recommend
    # you do set these to appropriate values for you.
    # However, for flexibility, we do allow the override of any pg_bouncer setting
    # many of which are vital to the operation of this helm chart.
    # The values we do not suggest altering are set in the template
    # https://github.com/timescale/helm-charts/blob/main/charts/timescaledb-single/templates/configmap-pgbouncer.yaml#L35-L50
    # Only override these settings if you are confident of  what you are doing.
      server_reset_query: DISCARD ALL
      max_client_conn: 500
      default_pool_size: 12
      pool_mode: transaction
    pg_hba:
    - local     all postgres                   peer
    - host      all postgres,standby 0.0.0.0/0 reject
    - host      all postgres,standby ::0/0     reject
    - hostssl   all all              0.0.0.0/0 md5
    - hostssl   all all              ::0/0     md5
    - hostnossl all all              0.0.0.0/0 reject
    - hostnossl all all              ::0/0     reject
    # Secret should contain user/password pairs in the format expected by pgbouncer
    # https://www.pgbouncer.org/config.html#authentication-file-format
    # example:
    # userlist.txt: |
    #   "username" "hashedpassword"
    #   "username2" "hashedpassword2"
    userListSecretName:

  networkPolicy:
    enabled: false
    prometheusApp: prometheus
    # Below you can specify a whitelist of Ingress rules, for more information:
    # https://kubernetes.io/docs/concepts/services-networking/network-policies/#the-networkpolicy-resource
    ingress:
    # - from:
    #   - podSelector:
    #       matchLabels:
    #         app: foo
    #   ports:
    #   - protocol: TCP
    #       port: 11111

  # Prometheus exporter for PostgreSQL server metrics.
  # https://github.com/prometheus-community/postgres_exporter
  prometheus:
    enabled: true
    image:
      repository: quay.io/prometheuscommunity/postgres-exporter
      tag: v0.11.1
      pullPolicy: Always
    # Extra custom environment variables for prometheus.
    # These should be an EnvVar, as this allows you to inject secrets into the environment
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#envvar-v1-core
    env:
    # - name: NOT_A_SECRET
    #   value: "test"
    # - name: MYAPPLICATION_STANDBY_PASSWORDS
    #   valueFrom:
    #     secretKeyRef:
    #       name: myapplication-passwords
    #       key: standby
    # Additional volumes for prometheus, e.g., to support additional queries.
    # These should be a Volume, as this allows you to inject any kind of Volume
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#volume-v1-core
    volumes:
    # - name: exporter-config
    #   configMap:
    #     name: exporter-prometheus
    #     items:
    #       - key: metrics_queries
    #         path: queries.yaml
    # Additional volume mounts, to be used in conjunction with the above variable.
    # https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#volumemount-v1-core
    volumeMounts:
    # - name: exporter-config
    #   mountPath: /var/exporter

  podMonitor:
    # Specifies whether PodMonitor for Prometheus operator should be created
    enabled: false
    path: /metrics
    interval: 10s
    # scrapeTimeout: 30s
    # Specifies namespace, where PodMonitor should be installed
    # namespace: monitoring
    # labels:
    #   release: prometheus
    # honorLabels: true
    # metricRelabelings: []
    # targetLabels:
    #   - foo

  # For new deployments, we would advise Parallel here, however as that change breaks previous
  # deployments, it is set to OrderedReady here
  podManagementPolicy: OrderedReady

  rbac:
    # Specifies whether RBAC resources should be created
    create: true

  serviceAccount:
    # Specifies whether a ServiceAccount should be created
    create: true
    # The name of the ServiceAccount to use.
    # If not set and create is true, a name is generated using the fullname template
    name:
    # A map of annotations to be set on the ServiceAccount
    annotations: {}

  debug:
    # This setting is mainly for during development, debugging or troubleshooting.
    # This command will be executed *before* the main container starts. In the
    # example below, we can mimick a slow restore by sleeping for 5 minutes before starting
    execStartPre:  # sleep 300

promscale:
  enabled: true
  replicaCount: 3
  extraArgs:
    - "--metrics.high-availability=true"
  #connectionSecretName: "tricorder-promscale-connection"
  connection:
    # Database connection settings. If `uri` is not
    # set then the specific user, pass, host, port and
    # sslMode properties are used.
    uri: ""
    # user used to connect to TimescaleDB
    user: "postgres"
    password: "tricorder"
    host: "timescaledb.{{ .Release.Namespace }}.svc.cluster.local"
    port: 5432
    sslMode: require
    # database name in which to store the metrics
    # must be created before start
    dbName: tsdb
  resources:
    requests:
      cpu: 50m
      memory: 500Mi
    limits:
      cpu: "1"
      memory: 1Gi

kube-prometheus-stack:
  enabled: true
  alertmanager:
    enabled: false
  grafana:
    enabled: true
    #defaultDashboardsTimezone: Asia/Shanghai
    adminPassword: tricorder
    image:
      repository: public.ecr.aws/w3q2q1s5/tricorder-frontend
      # Overrides the Grafana image tag whose default is the chart appVersion
      tag: 0.0.7
      sha: ''
      pullPolicy: Always
    resources:
      requests:
        cpu: 10m
        memory: 128Mi
      limits:
        cpu: "1"
        memory: 2Gi
    sidecar:
      datasources:
        enabled: true
        label: tricorder_datasource
        labelValue: "true"
        defaultDatasourceEnabled: false

      dashboards:
        multicluster:
          global:
            enabled: false
        enabled: true
        files:
          - dashboards/k8s-cluster.json
          - dashboards/k8s-hardware.json
          - dashboards/apm-dependencies.json
          - dashboards/apm-home.json
          - dashboards/apm-service-dependencies-downstream.json
          - dashboards/apm-service-dependencies-upstream.json
          - dashboards/apm-service-overview.json
          - dashboards/promscale.json
          - dashboards/postgres-overview.json
          - dashboards/postgres-details.json
          - dashboards/demo-ebpf-http.json
    persistence:
      type: pvc
      enabled: true
      accessModes:
        - ReadWriteOnce
    prometheus:
      datasource:
        enabled: true
        # By default url of data source is set to ts-prom connector instance
        # deployed with this chart. If a connector isn't used this should be
        # set to the prometheus-server.
        url: "http://{{ .Release.Name }}-promscale.{{ .Release.Namespace }}.svc:9201"
    timescale:
      datasource:
        enabled: true
        user: postgres
        # leaving password empty will cause helm to generate a random password
        pass: "tricorder"
        dbName: tsdb
        sslMode: require
        # By default the url/host is set to the db instance deployed
        # with this chart
        host: "timescaledb.{{ .Release.Namespace }}.svc"
        port: 5432
    jaeger:
      datasource:
        enabled: true
      # Endpoint for integrating jaeger datasource in grafana. This should point to HTTP endpoint, not gRPC.
      tricorderTracesQueryEndPoint: "{{ .Release.Name }}-promscale.{{ .Release.Namespace }}.svc:9201"

    additionalDataSources:
      - name: TimescaleDB-Tricorder
        uid: timescaledb_tricorder
        access: proxy
        editable: false
        type: postgres
        url: timescaledb:5432
        database: tricorder
        user: postgres
        jsonData:
          postgresVersion: 1400
          timescaledb: true
        secureJsonData:
          password: tricorder
        version: 1
    serviceMonitor:
      enabled: true
      labels:
        release: prometheus
      interval: ""
      tlsConfig: {}

  prometheus:
    enabled: true
    prometheusSpec:
      scrapeInterval: "1m"
      evaluationInterval: "1m"
      scrapeInterval: "10s"
      replicas: 2
      retention: 7d
      replicaExternalLabelName: "__replica__"
      # Promscale requires a cluster label to be present for high availability mode.
      prometheusExternalLabelName: "cluster"

      remoteWrite:
        - url: "http://{{ .Release.Name }}-promscale.{{ .Release.Namespace }}.svc:9201/write"
          remoteTimeout: 100s
          queueConfig:
            capacity: 100000
            maxSamplesPerSend: 10000
            batchSendDeadline: 30s
            minShards: 20
            maxShards: 20
            minBackoff: 100ms
            maxBackoff: 10s
      # Prometheus pod storage spec
      storageSpec:
        # Using PersistentVolumeClaim
        # disable mount sub path, use the root directory of pvc
        disableMountSubPath: true
        volumeClaimTemplate:
          spec:
            accessModes:
              - "ReadWriteOnce"
            resources:
              requests:
                storage: 2Gi
      resources:
        requests:
          cpu: 100m
          memory: 512Mi
        limits:
          cpu: "1"
          memory: 2048Mi
